{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b285bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as soup # https://realpython.com/beautiful-soup-web-scraper-python/\n",
    "import time\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcbedef",
   "metadata": {},
   "source": [
    "# GRAB ALL TICKERS INSIDE PRICE (MIN-MAX) RANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7007fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = []\n",
    "tickers_list = []\n",
    "tickers_group_list = []\n",
    "\n",
    "ticker_skip = 1900\n",
    "price_min = 5.00\n",
    "price_max = 25.00\n",
    "valid_page_status = True\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while (valid_page_status):    \n",
    "    \n",
    "    url =   f\"\"\"\n",
    "            https://www.marketwatch.com/tools/screener/stock?exchange=all&skip={ticker_skip}\n",
    "            &orderbyfield=&direction=desc\n",
    "            &visiblecolumns=Symbol,CompanyName,Price,NetChange,ChangePercent,Volume\n",
    "            &pricemin={price_min}&pricemax={price_max}\n",
    "            \"\"\" # must be inside while loop because {page_skip} won't update if outside of it...\n",
    "\n",
    "    page = requests.get(url)\n",
    "    soup_obj = soup(page.content, 'html.parser')\n",
    "    ticker_skip += 25\n",
    "    \n",
    "    cells_found = soup_obj.find(class_ = \"table__cell j-Symbol\")\n",
    "\n",
    "    if(cells_found):\n",
    "        pages.append(url)\n",
    "    else:\n",
    "        valid_page_status = False\n",
    "    \n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Length of Pages[]: {len(pages)}\")    \n",
    "print(f\"Elapsed Time: {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e991894",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in pages:\n",
    "    page = requests.get(item)\n",
    "\n",
    "    df = pd.read_html(page.content)\n",
    "\n",
    "    tickers = df[0]\n",
    "    tickers = tickers[\"Symbol\"]\n",
    "    \n",
    "    for t in tickers:\n",
    "        tickers_list.append(t)\n",
    "        \n",
    "    tickers_group_list.append(tickers)\n",
    "    \n",
    "\n",
    "import csv    \n",
    "today = date.today()\n",
    "date_format = today.strftime(\"%b-%d-%Y\")\n",
    "file_name = \"tickers_\" + date_format\n",
    "\n",
    "with open(file_name, 'w') as f:\n",
    "     \n",
    "    write = csv.writer(f)\n",
    "    write.writerows(tickers_group_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9347b7c",
   "metadata": {},
   "source": [
    "# GRAB PRICE CHART TABLES FOR EVERY TICKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71560805",
   "metadata": {},
   "outputs": [],
   "source": [
    "month = today.strftime(\"%B\").lower()\n",
    "year = today.strftime(\"%Y\")\n",
    "day = today.strftime(\"%d\")\n",
    "unix_time_yesterday = -1\n",
    "unix_time_5years = -1\n",
    "\n",
    "unix_converter_url = f\"https://timeconverter.online/list/{year}/{month}\"\n",
    "page = requests.get(unix_converter_url)\n",
    "soup_obj = soup(page.content, 'html.parser')\n",
    "table_rows = soup_obj.find_all(\"td\")\n",
    "\n",
    "# Get the unix time stamp for today's date\n",
    "for item in table_rows:\n",
    "    date_td = item.text.split(\" \")\n",
    "    if(date_td[0] == day):\n",
    "        unix_time_yesterday = item.find_previous().find_previous().text\n",
    "        # find_previous() written twice, since having been read already, the iterator has moved\n",
    "        \n",
    "year = str((int(year) - 5))\n",
    "unix_converter_url = f\"https://timeconverter.online/list/{year}/{month}\"\n",
    "page = requests.get(unix_converter_url)\n",
    "soup_obj = soup(page.content, 'html.parser')\n",
    "table_rows = soup_obj.find_all(\"td\") \n",
    "\n",
    "# Get the unix time stamp for date from 5 years ago\n",
    "for item in table_rows:\n",
    "    date_td = item.text.split(\" \")\n",
    "    if(date_td[0] == day):\n",
    "        unix_time_5years = item.find_previous().find_previous().text\n",
    "        \n",
    "main_url = f\"https://finance.yahoo.com/quote/{tickers_list[0]}?p={tickers_list[0]}\"\n",
    "        \n",
    "base_url = f\"https://finance.yahoo.com/quote/{tickers_list[0]}/history?p={tickers_list[0]}\"\n",
    "\n",
    "weekly_url =    f\"\"\"\n",
    "                https://finance.yahoo.com/quote/{tickers_list[0]}/history?\n",
    "                period1={unix_time_5years}&period2={unix_time_yesterday}\n",
    "                &interval=1wk&filter=history&frequency=1wk\n",
    "                &includeAdjustedClose=true    \n",
    "                \"\"\"\n",
    "\n",
    "# https://waterfrontonline.files.wordpress.com/2022/10/delistingrules.pdf\n",
    "compliance = \"https://listingcenter.nasdaq.com/noncompliantcompanylist.aspx\"\n",
    "\n",
    "#https://stackoverflow.com/questions/76115408/web-scrapping-yahoo-finance\n",
    "# credit goes to: https://stackoverflow.com/users/8743880/udhavsethi\n",
    "page = requests.get(weekly_url, headers={'user-agent': 'custom'})  # added: headers={'user-agent': 'custom'}\n",
    "\n",
    "df = pd.read_html(page.content)\n",
    "\n",
    "soup_obj = soup(page.content, 'html.parser')\n",
    "table_rows = soup_obj.find_all(\"td\")\n",
    "\n",
    "table_rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4f2631",
   "metadata": {},
   "source": [
    "# Stock Uprise Causes\n",
    "\n",
    "[company] Reports First Quarter 2023 Financial Results\n",
    "https://finance.yahoo.com/news/ainos-reports-first-quarter-2023-201500582.html\n",
    "\n",
    "\n",
    "\n",
    "https://finance.yahoo.com/news/nasdaq-approves-180-day-extension-113000124.html\n",
    "https://finance.yahoo.com/news/meiwu-technology-company-limited-received-130000250.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17b058c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siloa\\AppData\\Local\\Temp\\ipykernel_14880\\2628440690.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reduced_df[\"Price\"] = \"\"\n",
      "C:\\Users\\siloa\\AppData\\Local\\Temp\\ipykernel_14880\\2628440690.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reduced_df[\"Price\"][index] = price_found.text\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# compliance_list = []\n",
    "# filepath = os.path.join(\"NasdaqNonComplianceIssuers.csv\")\n",
    "# with open(filepath, encoding='utf') as csvfile:\n",
    "#     csvreader = csv.reader(csvfile, delimiter=\",\")\n",
    "#     csv_header = next(csvreader)    # Date, Profit/Losses\n",
    "#     for row in csvreader:\n",
    "#         print(row)\n",
    "#         compliance_list.append(row[1])  \n",
    "\n",
    "# https://listingcenter.nasdaq.com/NonCompliantCompanyList.aspx\n",
    "non_compliance_all_df = pd.read_csv ('NasdaqNonComplianceIssuers.csv')\n",
    "bid_price_df = non_compliance_all_df.loc[non_compliance_all_df['Deficiency'] == \"Bid Price\"]\n",
    "\n",
    "# define a new column\n",
    "reduced_df = bid_price_df\n",
    "reduced_df[\"Price\"] = \"\"\n",
    "\n",
    "reduced_df = bid_price_df[[\"Affected Issues\", \"Notification Date\", \"Price\"]]\n",
    "# only about half of the list is \"bid price\", so index and row values will not line up\n",
    "reduced_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Yahoo Finance only permits a limited amount of calls per hour, so break up the requests as needed...\n",
    "\n",
    "for index, row in reduced_df.iterrows():\n",
    "    \n",
    "    if(index < 50):\n",
    "        #grab the url to scrape\n",
    "        url = f'https://finance.yahoo.com/quote/{row[\"Affected Issues\"]}'\n",
    "        page = requests.get(url, headers={'user-agent': 'custom'})\n",
    "        soup_obj = soup(page.content, 'html.parser')\n",
    "        # https://stackoverflow.com/questions/52816683/beautiful-soup-find-element-with-multiple-classes\n",
    "        price_found = soup_obj.find(class_ = \"Fw(b) Fz(36px) Mb(-4px) D(ib)\")\n",
    "\n",
    "        # 'NoneType' object has no attribute 'text'\n",
    "        NoneType = type(None)\n",
    "        if(type(price_found) == NoneType):\n",
    "            reduced_df[\"Price\"][index] = price_found\n",
    "        else:\n",
    "            reduced_df[\"Price\"][index] = price_found.text     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26a28cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siloa\\AppData\\Local\\Temp\\ipykernel_14880\\1200540594.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reduced_df[\"Price\"][index] = price_found.text\n"
     ]
    }
   ],
   "source": [
    "for index, row in reduced_df.iterrows():\n",
    "    \n",
    "    if(index >= 50 and index < 100):\n",
    "        #grab the url to scrape\n",
    "        url = f'https://finance.yahoo.com/quote/{row[\"Affected Issues\"]}'\n",
    "        page = requests.get(url, headers={'user-agent': 'custom'})\n",
    "        soup_obj = soup(page.content, 'html.parser')\n",
    "        # https://stackoverflow.com/questions/52816683/beautiful-soup-find-element-with-multiple-classes\n",
    "        price_found = soup_obj.find(class_ = \"Fw(b) Fz(36px) Mb(-4px) D(ib)\")\n",
    "\n",
    "        # 'NoneType' object has no attribute 'text'\n",
    "        NoneType = type(None)\n",
    "        if(type(price_found) == NoneType):\n",
    "            reduced_df[\"Price\"][index] = price_found\n",
    "        else:\n",
    "            reduced_df[\"Price\"][index] = price_found.text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ed8df7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siloa\\AppData\\Local\\Temp\\ipykernel_14880\\230199425.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reduced_df[\"Price\"][index] = price_found.text\n"
     ]
    }
   ],
   "source": [
    "for index, row in reduced_df.iterrows():\n",
    "    \n",
    "    if(index >= 100 and index < 150):\n",
    "        #grab the url to scrape\n",
    "        url = f'https://finance.yahoo.com/quote/{row[\"Affected Issues\"]}'\n",
    "        page = requests.get(url, headers={'user-agent': 'custom'})\n",
    "        soup_obj = soup(page.content, 'html.parser')\n",
    "        # https://stackoverflow.com/questions/52816683/beautiful-soup-find-element-with-multiple-classes\n",
    "        price_found = soup_obj.find(class_ = \"Fw(b) Fz(36px) Mb(-4px) D(ib)\")\n",
    "\n",
    "        # 'NoneType' object has no attribute 'text'\n",
    "        NoneType = type(None)\n",
    "        if(type(price_found) == NoneType):\n",
    "            reduced_df[\"Price\"][index] = price_found\n",
    "        else:\n",
    "            reduced_df[\"Price\"][index] = price_found.text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c111fd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siloa\\AppData\\Local\\Temp\\ipykernel_14880\\4143852784.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reduced_df[\"Price\"][index] = price_found.text\n",
      "C:\\Users\\siloa\\AppData\\Local\\Temp\\ipykernel_14880\\4143852784.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reduced_df[\"Price\"][index] = price_found\n"
     ]
    }
   ],
   "source": [
    "for index, row in reduced_df.iterrows():\n",
    "    \n",
    "    if(index >= 150 and index < 200):\n",
    "        #grab the url to scrape\n",
    "        url = f'https://finance.yahoo.com/quote/{row[\"Affected Issues\"]}'\n",
    "        page = requests.get(url, headers={'user-agent': 'custom'})\n",
    "        soup_obj = soup(page.content, 'html.parser')\n",
    "        # https://stackoverflow.com/questions/52816683/beautiful-soup-find-element-with-multiple-classes\n",
    "        price_found = soup_obj.find(class_ = \"Fw(b) Fz(36px) Mb(-4px) D(ib)\")\n",
    "\n",
    "        # 'NoneType' object has no attribute 'text'\n",
    "        NoneType = type(None)\n",
    "        if(type(price_found) == NoneType):\n",
    "            reduced_df[\"Price\"][index] = price_found\n",
    "        else:\n",
    "            reduced_df[\"Price\"][index] = price_found.text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87b34610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siloa\\AppData\\Local\\Temp\\ipykernel_14880\\91477333.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reduced_df[\"Price\"][index] = price_found\n",
      "C:\\Users\\siloa\\AppData\\Local\\Temp\\ipykernel_14880\\91477333.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reduced_df[\"Price\"][index] = price_found.text\n"
     ]
    }
   ],
   "source": [
    "for index, row in reduced_df.iterrows():\n",
    "    \n",
    "    if(index >= 200 and index < 250):\n",
    "        #grab the url to scrape\n",
    "        url = f'https://finance.yahoo.com/quote/{row[\"Affected Issues\"]}'\n",
    "        page = requests.get(url, headers={'user-agent': 'custom'})\n",
    "        soup_obj = soup(page.content, 'html.parser')\n",
    "        # https://stackoverflow.com/questions/52816683/beautiful-soup-find-element-with-multiple-classes\n",
    "        price_found = soup_obj.find(class_ = \"Fw(b) Fz(36px) Mb(-4px) D(ib)\")\n",
    "\n",
    "        # 'NoneType' object has no attribute 'text'\n",
    "        NoneType = type(None)\n",
    "        if(type(price_found) == NoneType):\n",
    "            reduced_df[\"Price\"][index] = price_found\n",
    "        else:\n",
    "            reduced_df[\"Price\"][index] = price_found.text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "947a7c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siloa\\AppData\\Local\\Temp\\ipykernel_14880\\2577718552.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reduced_df[\"Price\"][index] = price_found.text\n"
     ]
    }
   ],
   "source": [
    "for index, row in reduced_df.iterrows():\n",
    "    \n",
    "    if(index >= 250 and index < 300):\n",
    "        #grab the url to scrape\n",
    "        url = f'https://finance.yahoo.com/quote/{row[\"Affected Issues\"]}'\n",
    "        page = requests.get(url, headers={'user-agent': 'custom'})\n",
    "        soup_obj = soup(page.content, 'html.parser')\n",
    "        # https://stackoverflow.com/questions/52816683/beautiful-soup-find-element-with-multiple-classes\n",
    "        price_found = soup_obj.find(class_ = \"Fw(b) Fz(36px) Mb(-4px) D(ib)\")\n",
    "\n",
    "        # 'NoneType' object has no attribute 'text'\n",
    "        NoneType = type(None)\n",
    "        if(type(price_found) == NoneType):\n",
    "            reduced_df[\"Price\"][index] = price_found\n",
    "        else:\n",
    "            reduced_df[\"Price\"][index] = price_found.text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08decf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siloa\\AppData\\Local\\Temp\\ipykernel_14880\\2881877909.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reduced_df[\"Price\"][index] = price_found\n",
      "C:\\Users\\siloa\\AppData\\Local\\Temp\\ipykernel_14880\\2881877909.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reduced_df[\"Price\"][index] = price_found.text\n"
     ]
    }
   ],
   "source": [
    "for index, row in reduced_df.iterrows():\n",
    "    \n",
    "    if(index >= 300 and index < 350):\n",
    "        #grab the url to scrape\n",
    "        url = f'https://finance.yahoo.com/quote/{row[\"Affected Issues\"]}'\n",
    "        page = requests.get(url, headers={'user-agent': 'custom'})\n",
    "        soup_obj = soup(page.content, 'html.parser')\n",
    "        # https://stackoverflow.com/questions/52816683/beautiful-soup-find-element-with-multiple-classes\n",
    "        price_found = soup_obj.find(class_ = \"Fw(b) Fz(36px) Mb(-4px) D(ib)\")\n",
    "\n",
    "        # 'NoneType' object has no attribute 'text'\n",
    "        NoneType = type(None)\n",
    "        if(type(price_found) == NoneType):\n",
    "            reduced_df[\"Price\"][index] = price_found\n",
    "        else:\n",
    "            reduced_df[\"Price\"][index] = price_found.text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c08f034",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in reduced_df.iterrows():\n",
    "    \n",
    "    if(index >= 350 and index < 400):\n",
    "        #grab the url to scrape\n",
    "        url = f'https://finance.yahoo.com/quote/{row[\"Affected Issues\"]}'\n",
    "        page = requests.get(url, headers={'user-agent': 'custom'})\n",
    "        soup_obj = soup(page.content, 'html.parser')\n",
    "        # https://stackoverflow.com/questions/52816683/beautiful-soup-find-element-with-multiple-classes\n",
    "        price_found = soup_obj.find(class_ = \"Fw(b) Fz(36px) Mb(-4px) D(ib)\")\n",
    "\n",
    "        # 'NoneType' object has no attribute 'text'\n",
    "        NoneType = type(None)\n",
    "        if(type(price_found) == NoneType):\n",
    "            reduced_df[\"Price\"][index] = price_found\n",
    "        else:\n",
    "            reduced_df[\"Price\"][index] = price_found.text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1191da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in reduced_df.iterrows():\n",
    "    \n",
    "    if(index >= 400 and index < 450):\n",
    "        #grab the url to scrape\n",
    "        url = f'https://finance.yahoo.com/quote/{row[\"Affected Issues\"]}'\n",
    "        page = requests.get(url, headers={'user-agent': 'custom'})\n",
    "        soup_obj = soup(page.content, 'html.parser')\n",
    "        # https://stackoverflow.com/questions/52816683/beautiful-soup-find-element-with-multiple-classes\n",
    "        price_found = soup_obj.find(class_ = \"Fw(b) Fz(36px) Mb(-4px) D(ib)\")\n",
    "\n",
    "        # 'NoneType' object has no attribute 'text'\n",
    "        NoneType = type(None)\n",
    "        if(type(price_found) == NoneType):\n",
    "            reduced_df[\"Price\"][index] = price_found\n",
    "        else:\n",
    "            reduced_df[\"Price\"][index] = price_found.text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8c5304",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in reduced_df.iterrows():\n",
    "    \n",
    "    if(index >= 450 and index < 500):\n",
    "        #grab the url to scrape\n",
    "        url = f'https://finance.yahoo.com/quote/{row[\"Affected Issues\"]}'\n",
    "        page = requests.get(url, headers={'user-agent': 'custom'})\n",
    "        soup_obj = soup(page.content, 'html.parser')\n",
    "        # https://stackoverflow.com/questions/52816683/beautiful-soup-find-element-with-multiple-classes\n",
    "        price_found = soup_obj.find(class_ = \"Fw(b) Fz(36px) Mb(-4px) D(ib)\")\n",
    "\n",
    "        # 'NoneType' object has no attribute 'text'\n",
    "        NoneType = type(None)\n",
    "        if(type(price_found) == NoneType):\n",
    "            reduced_df[\"Price\"][index] = price_found\n",
    "        else:\n",
    "            reduced_df[\"Price\"][index] = price_found.text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "335ae462",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = reduced_df.sort_values(['Price'], ascending=[True])\n",
    "sorted_df.to_csv(\"temp_list_sorted\", sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d605c7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
